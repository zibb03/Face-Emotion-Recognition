월 수 AI 님과 카카오톡 대화
저장한 날짜 : 2020-07-08 21:39:15

--------------- 2020년 4월 13일 월요일 ---------------
[김지훈] [오후 8:30] https://m.blog.naver.com/PostView.nhn?blogId=roboholic84&logNo=221633210887&proxyReferer=https:%2F%2Fwww.google.com%2F
[정종민] [오후 8:56] 제가 지난번에 올려드렸던 사진
[정종민] [오후 8:56] 혹시 알아보신분 있나요
[원종윤] [오후 8:57] 저는 반전이미지로 컨투어 해보느라 그 알고리즘은 아직 안했습니다
[정종민] [오후 8:57] 선생님께서 컨투어는 얼굴과 전경 분리에 어려움이 있다고 하셨으니까
[정종민] [오후 8:58] python에서 얼굴과 전경 분리에 사용되는 알고리즘 위주로 조사하면 좋을꺼같아요
[정종민] [오후 8:58] grab cut이 제일 무난할듯 싶은데
[원종윤] [오후 8:58] 그럼 제가 grab cut 찾아보겠습니다.
[정종민] [오후 9:20] 사진
[정종민] [오후 9:20] 사진
[정종민] [오후 9:20] grabcut쓰면 될꺼같습니다
[정종민] [오후 9:20] 잘 구별되네요
--------------- 2020년 4월 20일 월요일 ---------------
[김지훈] [오후 8:24] 사진
[김지훈] [오후 8:24] 감정인식 되는거같아요
[김지훈] [오후 8:24] 깃헙 누구 만들고 계신가요?
[정종민] [오후 8:25] 오...
[정종민] [오후 8:25] 저거 코드 알려주세요
[정종민] [오후 8:25] 제가 만들어볼게요
[김지훈] [오후 8:25] 파일: 감정인식.txt
[김지훈] [오후 8:25] 파일: emotion_model.zip
[김지훈] [오후 8:25] 감정인식 모델파일 이에요 저건
[정종민] [오후 8:42] 각자
[정종민] [오후 8:42] 깃허브 아이디나
[정종민] [오후 8:42] 이메일 알려주세요
[김지훈] [오후 8:42] jihunsg
[원종윤] [오후 8:42] noel8044@naver.com이요
[찬울] [오후 8:42] ohchanooel
[최연석] [오후 8:43] enixjm
[정종민] [오후 9:18] grabcut 막히는데
[정종민] [오후 9:18] http://www.gisdeveloper.co.kr/?p=6747
[정종민] [오후 9:18] 이거 본인이미지나 다른사람 얼굴 해서
[정종민] [오후 9:18] 틀 따보실수 있는분 계신가요..
[김지훈] [오후 9:18] 해볼게요
[김지훈] [오후 9:29] 지금 감정인식 한거 
[김지훈] [오후 9:29] 푸쉬 해볼게요
[김지훈] [오후 9:31] 푸쉬 했어요
[김지훈] [오후 9:37] 사진
[김지훈] [오후 9:37] 이러면 되는건가요
[정종민] [오후 9:37] 원본
[정종민] [오후 9:37] 보여주실수 있나요?
[김지훈] [오후 9:37] 사진
[정종민] [오후 9:38] 그 머리쪽이 짤렸잖아요
[정종민] [오후 9:38] 좀더 정확하게 하려면 new mask라고 그걸 써야되는데
[정종민] [오후 9:38] 그거 하기가 쉽지 않네요
[김지훈] [오후 9:38] step2 에서 newmask 는 뭐에요?
[정종민] [오후 9:39] 아래에
[정종민] [오후 9:39] 이상하게 생긴 회색바탕에
[정종민] [오후 9:39] 검은색 회색 선해둔거 있잖아요
[정종민] [오후 9:39] 사진
[정종민] [오후 9:39] 1차적으로 딴다음
[정종민] [오후 9:39] 위 사진처럼 흰색과 검은색 바탕을 구분해서 
[정종민] [오후 9:39] 2차적으로 처리하는게 필요한거 같앙요
[정종민] [오후 9:39] 같아요
[김지훈] [오후 9:40] 흠 어럽네요
[정종민] [오후 9:40] ㅋㅋㅋ 저도 잘 모르겠어요...
[정종민] [오후 9:40] 지금 테두리 알아보시는 분들은
[정종민] [오후 9:40] 다른 좋은 얼굴테두리 따는법 있으면
[정종민] [오후 9:41] 좀 알려주세요
[원종윤] [오후 9:41] new mask가 덜 잘린 부분 자르는 과정에서 사용되는거지요..?
[정종민] [오후 9:41] 네네
[정종민] [오후 9:42] 검은색 배경쪽 덜잘린 부분은 검은색 라인으로 칠해주고
[정종민] [오후 9:42] 딸 얼굴이나 물체가 안잘리면 흰색으로 칠해주는거 같아요
[정종민] [오후 9:42] 근데 이게 이미지 크기에 따라 까다롭게 반응해서 쓰기가 어렵네요;;
--------------- 2020년 4월 22일 수요일 ---------------
[정종민] [오후 8:18] 다들 지금 뭐하세요?
[원종윤] [오후 8:18] new마스크 해보고있어요
[김지훈] [오후 8:18] 전 감정인식 하는거 만져보고있어요 
[찬울] [오후 8:19] 저도 newmask 하고있어요
[정종민] [오후 8:20] newmask 하시는분들
[정종민] [오후 8:20] 그랩컷 어떤느낌인지 알겠나요
[정종민] [오후 8:20] ?
[찬울] [오후 8:21] 어느정도는 알겠는데 조금 더 해봐야 익숙할거같아요
[원종윤] [오후 8:22] 혹시 newmask 성공하신분 계시나요?
[김지훈] [오후 8:24] 근데 저희 혹시 영상으로 안하고 사진한장 가지고 감정인식하고 배경바꾸고 다하나요?
[김지훈] [오후 8:24] 저 newmask코드를 영상으로 해보니까 프레임이 너무 끊기던데
[정종민] [오후 8:25] 저희 목표가
[정종민] [오후 8:25] 사진찍어서 감정인식한다음에 배경바꾸는거
[정종민] [오후 8:25] 아니었나요?
[김지훈] [오후 8:25] 카메라 켜져있는 상태에서는 아무것도 안하는건가요 그럼?
[정종민] [오후 8:26]  그게 무슨 뜻이죠
[김지훈] [오후 8:26] 카메라 켜져있는 상태에서 사진을 찍고 나서 얼굴이랑 감정인식 하는거죠?
[정종민] [오후 8:27] 아까 영상 애기하신건
[정종민] [오후 8:27] 무슨 의미셨어요?
[김지훈] [오후 8:28] 카메라 켜져있을때 화면에 실시간으로 감정인식 하면서 배경바꾸는걸 말한거에요
[정종민] [오후 8:28] 해도 되긴 하는데
[정종민] [오후 8:29] 지금 사진도 못하는데 움직이는걸 하는게 가능할까요?
[정종민] [오후 8:29] 어짜피 그랩컷 응용하면 동영상도 되는걸로 알고있습니다
[김지훈] [오후 8:30] 그럼 일단 사진부터 하는걸로 하고 영상도 할수있으면 하는걸로 할까요?
[정종민] [오후 8:30] 네 뭐
[정종민] [오후 8:30] 사진으로 완벽하게 만들정도면
[정종민] [오후 8:30] 동영상으로 응용시키는거야 어렵지 않다고 봅니다
[김지훈] [오후 8:31] 알겠습니다
[정종민] [오후 9:49] 지금
[정종민] [오후 9:51] 저희 얼굴만 따는건가요
[정종민] [오후 9:51] 아니면 몸부위는 다 따는건가요
[정종민] [오후 9:51] 처음에 스노우카메라처럼 만들겠다고 하셨으면 몸통까지 따는건가요?
[원종윤] [오후 9:52] 그리고 저희 1명 얼굴만 따게 되는건가요..?
[김지훈] [오후 9:52] 음 일단 하기 쉬운거 부터 해도 될거같아요
[정종민] [오후 9:52] 그러면
[김지훈] [오후 9:52] 정면사진이면 얼굴은 잘 인식되니까
[정종민] [오후 9:52] 셀카처럼
[정종민] [오후 9:52] 한명만 얼굴 따는거부터 시작해서
[정종민] [오후 9:52] 그거 잘되면 다른것도 시도해보죠
[김지훈] [오후 9:53] 사진
[김지훈] [오후 9:53] 여러명도
[김지훈] [오후 9:53] 해봐쓴데 잘 돼요
[김지훈] [오후 9:53] 근데 표정이랑 결과가 좀 이상하긴해요
[정종민] [오후 9:54] 한명을 지정해서 하면
[정종민] [오후 9:54] 좀 더 정확히 나오나요?
[김지훈] [오후 9:54] 아뇨 다 똑같아요
[김지훈] [오후 9:55] 그리고 얼굴인식 은
[김지훈] [오후 9:55] dlib 사용해서 해야될거같아요
[김지훈] [오후 9:55] 그 haarcascade 쓰니까 잘안되더라구요
[정종민] [오후 9:55] 전경분리 끝나는대로
[김지훈] [오후 9:55] 저 코드는 정리해서 내일까지 깃헙에 올려놓을게요
[정종민] [오후 9:56] 저도 빨리 감정인식 알아볼게요
[원종윤] [오후 9:56] 인식률이 낮아서 그런건가요 아니면 감정인식코드랑랑 관련이 있는건가요?
[김지훈] [오후 9:56] 인식률이 낮아요
[김지훈] [오후 9:56] 얼굴을 인식하고 감정을 인식하는데
[김지훈] [오후 9:56] 얼굴이 인식이 안되더라구요
--------------- 2020년 4월 23일 목요일 ---------------
[김지훈] [오후 1:59] 사진
[김지훈] [오후 1:59] 어제 감정인식 잘안되던거 수정해서 깃헙에 코드 주석도 같이 달아서 올려놨어요 
--------------- 2020년 4월 27일 월요일 ---------------
[김지훈] [오후 8:15] 혹시 테두리따는거 하신분 있나요
[정종민] [오후 8:16] 지금 하고있는데
[정종민] [오후 8:16] 얼굴인식 코드좀 올려주세요
[정종민] [오후 8:19] import numpy as np
import cv2

BLUE, GREEN, RED, BLACK, WHITE = (255,0,0),(0,255,0), (0,0,255), (0,0,0), (255,255,255)
DRAW_BG = {'color':BLACK, 'val':0}
DRAW_FG = {'color':WHITE, 'val':1}

rect = (0,0,1,1)
drawing = False
rectangle = False
rect_over = False
rect_or_mask = 100
value = DRAW_FG
thickness = 3

def onMouse(event, x, y, flags, param):
    global ix, iy, img, img2, drawing, value, mask, rectangle
    global rect, rect_or_mask, rect_over

    if event == cv2.EVENT_RBUTTONDOWN:
        rectangle = True
        ix, iy = x, y
    elif event == cv2.EVENT_MOUSEMOVE:
        if rectangle:
            img = img2.copy()
            cv2.rectangle(img,(ix,iy),(x,y),RED,2)
            rect = (min(ix,x),min(iy,y),abs(ix-x),abs(iy-y))
            rect_or_mask = 0
    elif event == cv2.EVENT_RBUTTONUP:
        rectangle = False
        rect_over = True

        cv2.rectangle(img,(ix,iy),(x,y),RED,2)
        rect = (min(ix,x),min(iy,y),abs(ix-x),abs(iy-y))
        rect_or_mask = 0
        print('n:적용하기')

    if event == cv2.EVENT_LBUTTONDOWN:
        if not rect_over:
            print('마우스 왼쪽 버튼을 누른채로 전경이 되는 부분을 선택하세요')
        else:
            drawing = True
            cv2.circle(img,(x,y),thickness, value['color'], -1)
            cv2.circle(mask(x,y),thickness, value['val'], -1)

    elif event == cv2.EVENT_MOUSEMOVE:
        if drawing:
            cv2.circle(img, (x,y), thickness, value['color'], -1)
            cv2.circle(mask, (x,y),thickness, value['val'],-1)
    elif event == cv2.EVENT_LBUTTONUP:
        if drawing:
            drawnig = False
            cv2.circle(img,(x,y),thickness, value['color'],-1)
            cv2.circle(mask,(x,y),thickness, valuw['var'],-1)
    return


def grabcut():
    global ix, iy, img, img2, drawing, value, mask, rectangle
    global rect, rect_or_mask, rect_over

    img = cv2.imread('C:/Users/jongmin/Desktop/myface.jpg')
    img2 = img.copy()

    mask = np.zeros(img.shape[:2], dtype=np.uint8)
    output = np.zeros(img.shape, np.uint8)

    cv2.namedWindow('input')
    cv2.namedWindow('output')
    cv2.setMouseCallback('input', onMouse, param=(img, img2))
    cv2.moveWindow('input', img.shape[1]+10, 90)

    print('오른쪽 마우스 버튼을 누르고 영역을 지정한 후 n을 누르세요0')

    while True:
        cv2.imshow('output',output)
        cv2.imshow('input', img)

        k = cv2.waitKey(1) & 0xFF

        if k == 27:
            break
        if k == ord('0'):
            print('왼쪽 마우스로 제거할 부분을 표시한 후 n을 누르세요')
        elif k == ord('1'):
            print('왼쪽 마우스로 복원할 부분을 표시한 후 n을 누르세요')
            value = DRAW_FG
        elif k == ord('r'):
            print('리셋합니다')
            rect(0,0,1,1)
            drawing = False
            rectangle = False
            rect_or_mask = 100
            rect_over = False
            value = DRAW_FG
            img = img2.copy()
            mask = np.zeros(img.shape[:2], dtype=np.uint8)
            output = np.zeros(img.shape,np.uint8)
            print('0:제거배경선택 1:복원전경선택 n:적용하기 r:리셋')
        elif k == ord('n'):
            bgdModel = np.zeros((1,65), np.float64)
            fgdModel = np.zeros((1,65), np.float64)

            if rect_or_mask==0:
                cv2.grabCut(img2, mask, rect,bgdModel, fgdModel, 1, cv2.GC_INIT_WITH_RECT)
                rect_or_mask = 1
            elif rect_or_mask == 1:
                cv2.grabCut(img2, mask, rect, bgdModel, fgdModel, 1 , cv2.GC_INIT_WITH_MASK)

            print('0:제거배경선택 1:복원전경선택 n:적용하기 r:리셋')

        mask2 = np.where((mask==1) + (mask==3),255,0).astype('uint8')
        output = cv2.bitwise_and(img2, img2, mask=mask2)

    cv2.destroyWindows()


grabcut()
[정종민] [오후 8:19] 이제 코딩을 해야하는데
[정종민] [오후 8:19] 위 코드가 바탕화면에 myface.jpg넣으면 얼굴테두리 짤라주는 코드입니다
[정종민] [오후 8:19] 얼굴인식이랑 연동시켜야 하는데
[정종민] [오후 8:20] 파이썬 잘하시는분이나 연결시킬 아이디어 있으신분 계신가요
[정종민] [오후 8:23] https://cafe.naver.com/opencv/52764
[정종민] [오후 8:23] 지금 제 고민이 이글이랑 비슷한거 같은데
[정종민] [오후 8:23] https://blog.naver.com/jmee9708/221489895270
[정종민] [오후 8:23] 사람인식이에요
[정종민] [오후 8:23] 아니면 얼굴인식이에요?
[원종윤] [오후 8:23] 카페 가입안하면 못봐요..
[정종민] [오후 8:24] 네이버에 얼굴인식코드치고 아래 내리면
[정종민] [오후 8:24] 나옵니다
[원종윤] [오후 8:24] 넵
[김지훈] [오후 8:25] 저 코드 혹시
[김지훈] [오후 8:25] 결과나온거좀 캡쳐
[김지훈] [오후 8:25] 해주실수있나요
[원종윤] [오후 8:36] dlib 코드 찾고 있었는데, 됬던 코드가 공백문제로 안되서 다시 수정하고 있습니다.
[정종민] [오후 8:36] 얼굴인식 하시는분은 계속 하시고요
[정종민] [오후 8:36] 아니 감정인식
[정종민] [오후 8:36] 얼굴 테두리 따시는 세분은
[정종민] [오후 8:37] 제가 올려드린 코드랑 얼굴인식 코드랑 합칠준비 하셔야 할것 같습니다
[최연석] [오후 8:37] 예
[김지훈] [오후 8:37] 저도 지금 위에 코드랑 감정인식코드 합쳐보고있어요
[원종윤] [오후 8:38] 저 코드 지금 수정중인데, 흰 배경 이미지 넣어도, 맨 처음에 영역은 무조건 지정해줘야하고, 흰 배경에 영역까지 정해줬음에도 불구하고 테두리 주변을 수동으로 그려야 깔끔하게 짤려요..
[정종민] [오후 8:38] 얼굴인식하면 주변에 테두리 뜨잖아요
[정종민] [오후 8:38] 그게 위 코드의 지정사각형이 될껍니다
[김지훈] [오후 8:39] 저 위에 테두리따는 코드에서
[김지훈] [오후 8:39] 직접 그려서 영역 따는 부분은
[김지훈] [오후 8:39] 지워야 되지 않을까요
[정종민] [오후 8:39] 그래서 
[정종민] [오후 8:39] 코드수정이 필요한데
[정종민] [오후 8:39] 일단 얼굴인식 코드 찾아가지고 수정해야죠
[김지훈] [오후 8:39] 그
[김지훈] [오후 8:39] 제가 깃헙에 올린 코드
[김지훈] [오후 8:39] 보셨나요
[정종민] [오후 8:40] 감정인식이요?
[김지훈] [오후 8:40] Test2-dlib 파일에서
[김지훈] [오후 8:40] 27~30번째줄이
[김지훈] [오후 8:40] 얼굴 인식된
[김지훈] [오후 8:40] 좌표에ㅛㅇ
[정종민] [오후 8:41] 감정인식에
[정종민] [오후 8:41] 얼굴인식이 포함되어있나요
[김지훈] [오후 8:41] 네 얼굴을 먼저 인식하고
[김지훈] [오후 8:41] 감정을 인식해야되요
[정종민] [오후 8:41] 그러면
[정종민] [오후 8:41] faceemotion이랑
[정종민] [오후 8:41] 위 코드 합치기만 하면 되는건가요
[김지훈] [오후 8:41] 네 그러면 될거에요
[정종민] [오후 8:42] 알겠습니다
[정종민] [오후 8:42] 코드 합치는데
[정종민] [오후 8:42] 각각 역할분담은 어떤씩으로 해야할까요?
[김지훈] [오후 8:43] 음 어떡할까요 역할분담
[정종민] [오후 8:44] 이제 조사는 끝났으니
[정종민] [오후 8:44] 좀 부족하더라도 일차적으로 완성품 내는게 중요하다고 생각합니다
[정종민] [오후 8:44] 그걸 중심으로 차차 수정해가면 될꺼같아요
[김지훈] [오후 8:44] 저희가 배경까지 합쳐야 하니까
[김지훈] [오후 8:44] 몇명은 배경합치는 부분
[김지훈] [오후 8:44] 찾아보는게 어떨깡
[김지훈] [오후 8:44] 요
[정종민] [오후 8:46] 예 그럼
[정종민] [오후 8:47] 지훈님이랑 저랑 코드 합쳐볼테니 나머지 두분께서 배경찾기 해주시는게 좋을꺼 같습니다
[정종민] [오후 8:47] 지금 하는게 서로 불명확해서
[정종민] [오후 8:48] 확실히 잡는게 좋을꺼같아요
[정종민] [오후 8:48] 배경붙여넣는것도
[정종민] [오후 8:48] 에초에 목표가 ai써서 감정에 따라 해야하므로
[정종민] [오후 8:48] 그에 관해 조사도 좀 해주시고요
[정종민] [오후 8:49] 코드 쓸만한거 있으면 정리해서 남겨주심 감사하겠습니다
[원종윤] [오후 8:50] 감정인식 코드에 결과에 따라 이미지를 주는배경이미지 라이브러리랑, 사람 몸통 이미지 자른 걸로 이미지 합성하는거 알아보면 되는 건가요..?
[정종민] [오후 8:50] 네네
[정종민] [오후 8:50] 그만하면 충분한데
[정종민] [오후 8:51] ai는 텐서플로우같은거는 막 어려우니까 감정선이 happy sad 뭐 이런걸로 정해져 있잖아요
[정종민] [오후 8:51] 경우의 수를 지정해서 하는게 나을꺼 같습니다 그때마다 배경찾는건 처음에 하기 힘들꺼같아요
[원종윤] [오후 8:54] 그 감정인식 코드 결과가 여려가지인데, 그 결과값에서 비슷한 몇개를 묶어서 그 집단에 따라 이미지가 나오게 하라는 말씀이신가요..?
[정종민] [오후 8:54] 음
[정종민] [오후 8:54] 간단하게 말하자면
[정종민] [오후 8:54] sad는 우울하니까
[정종민] [오후 8:54] 파란색 이미지
[정종민] [오후 8:54] 초반에 ai로 바로 접목시키긴 힘드니까 이런씩으로 가는게 더 수월할꺼 같습니다
[정종민] [오후 8:54] 물런 여유가 된다면 인공지능도 활용해보고요
[원종윤] [오후 8:55] 일단 알겠습니다
--------------- 2020년 4월 29일 수요일 ---------------
[원종윤] [오후 9:11] 이미지 합성은 성공했습니다. 근데 전제 조건이 배경이 검정색이여야 합니다.
[정종민] [오후 9:11] 어떻게 하셨어요?
[원종윤] [오후 9:12] Roi 써서 했습니다 잠시만요
[원종윤] [오후 9:12] import cv2
import numpy as np

 # 이미지 지정
background = cv2.imread("C:/Users/user/PycharmProjects/OpenCV/party.jpg")
logo = cv2.imread("C:/Users/user/PycharmProjects/OpenCV/Black Layer.png")

gray_logo = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)
_, mask_inv = cv2.threshold(gray_logo, 10, 255, cv2.THRESH_BINARY_INV)

cv2.imshow("mask_inv", mask_inv)
cv2.waitKey()

background_height, background_width, _ = background.shape # 900, 600, 3
logo_height, logo_width, _ = logo.shape # 360, 313, 3

x = (background_height - logo_height) //2
'
y = (background_width - logo_width) // 2


roi = background[x: x+logo_height, y: y+logo_width]

cv2.imshow("ROI", roi)
cv2.waitKey()

roi_logo = cv2.add(logo, roi, mask=mask_inv)
cv2.imshow("roi_logo", roi_logo)
cv2.waitKey()

result = cv2.add(roi_logo, logo)
cv2.imshow("result", result)
cv2.waitKey()

np.copyto(roi, result)
cv2.imshow("result_background", background)
cv2.waitKey()
[정종민] [오후 9:12] Grabcut 쓰면 검은색되니까
[정종민] [오후 9:12] 합쳐보겠습니다
[원종윤] [오후 9:21] x 정의된 부분만 이렇게 바꿔주세요 이렇게 해야 사진이 밑에 붙어있어요 -> x = background_height - logo_height 
[정종민] [오후 9:23] 사진
[정종민] [오후 9:23] grabcut이랑 합치는거
[정종민] [오후 9:23] 성공했씁니다
[정종민] [오후 9:23] 빨간배경이랑 사람얼굴이랑 가져온 뒤에
[정종민] [오후 9:23] 사진
[정종민] [오후 9:24] 사진
[정종민] [오후 9:24] grabcut으로 사람얼굴 구별하고
[정종민] [오후 9:24] 거기에 배경 붙이는거까지 끝냈어요
[정종민] [오후 9:24] import cv2
import numpy as np
from keras.preprocessing.image import img_to_array
from keras.models import load_model
import dlib
#dlib를 상용 하는 이유는 XML파일을 이용하는 것 보다 dlib를 사용하여 인식 하는 것이 더 인식이 잘되는 듯 함
#dlib를 설치하기 위해서는 Anaconda에서 pip install cmake -> pip install dlib 설치


# dlib얼굴 인식 모델, 감정인식 모델 불러오기
#face_detection = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml') #사용 안함
detector = dlib.get_frontal_face_detector() #얼굴 인식을 위한 dlib 기본 안면인식 모델
emotion_classifier = load_model('./emotion/emotion_model.hdf5', compile=False)
EMOTIONS = ["Angry", "Disgusting", "Fearful", "Happy", "Sad", "Surpring", "Neutral"]


# 이미지 불러오기
frame = cv2.imread('./SampleImage/peoples.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((1200, 720, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

background_sad = cv2.imread('./SampleImage/background_sad.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((600, 500, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

background_happy = cv2.imread('./SampleImage/background_happy.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((600, 500, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

# Perform emotion recognition only when face is detected
if len(dets) > 0:
    # Resize the image to 48x48 for neural network
    for k, d in enumerate(dets): # k = 얼굴 인식 갯수, d = 좌표(?)
        fX = d.left()
        fY = d.top()
        fW = d.right()
        fH = d.bottom()
        print(fX, fY, fW, fH)

        # grabcut
        rect = (fX, int((fY - fY / 2)), fW, fH)
        img = frame.copy()
        img2 = img.copy()
        mask = np.zeros(img.shape[:2], dtype=np.uint8)
        output = np.zeros(img.shape, np.uint8)

        bgdModel = np.zeros((1, 65), np.float64)
        fgdModel = np.zeros((1, 65), np.float64)

        cv2.grabCut(img2, mask, rect, bgdModel, fgdModel, 1, cv2.GC_INIT_WITH_RECT)
        mask2 = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')
        output = cv2.bitwise_and(img2, img2, mask=mask2)


        # --grabcut

        roi = gray[fY:fH, fX:fW] #얼굴 인식된 좌표
        roi = cv2.resize(roi, (64, 64))
        roi = roi.astype("float") / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)

        # Emotion predict
        #감정 인식하기
        preds = emotion_classifier.predict(roi)[0]
        emotion_probability = np.max(preds)
        label = EMOTIONS[preds.argmax()]

        # Assign labeling
        cv2.putText(frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
        cv2.rectangle(frame, (fX, fY), (fW, fH), (0, 0, 255), 2)

    # Label printing
    #사람이 여려명일 때는 마지막으로 감정을 인식한 사람의 감정 상태만 프린트함
    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):
        text = "{}: {:.2f}%".format(emotion, prob * 100)
        w = int(prob * 300)
        cv2.rectangle(canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 0, 255), -1)
        cv2.putText(canvas, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)


# 이미지 지정
background = cv2.imread("./SampleImage/background_happy.jpg")
logo = output

gray_logo = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)
_, mask_inv = cv2.threshold(gray_logo, 10, 255, cv2.THRESH_BINARY_INV)

cv2.imshow("mask_inv", mask_inv)
cv2.waitKey()

background_height, background_width, _ = background.shape # 900, 600, 3
logo_height, logo_width, _ = logo.shape # 360, 313, 3

x = (background_height - logo_height) //2
y = (background_width - logo_width) // 2


roi = background[x: x+logo_height, y: y+logo_width]

cv2.imshow("ROI", roi)
cv2.waitKey()

roi_logo = cv2.add(logo, roi, mask=mask_inv)
cv2.imshow("roi_logo", roi_logo)
cv2.waitKey()

result = cv2.add(roi_logo, logo)
cv2.imshow("result", result)
cv2.waitKey()

np.copyto(roi, result)
cv2.imshow("result_background", background)
cv2.waitKey()


# Open two windows
## Display image ("Emotion Recognition")
## Display probabilities of emotion
cv2.imshow('output', output)
cv2.imshow('Emotion Recognition', frame)
cv2.imshow("Probabilities", canvas)
cv2.waitKey(0)



# Clear program and close windows
cv2.destroyAllWindows()
[정종민] [오후 9:24] github에 올려두겠습니다
[정종민] [오후 9:55] 프로토타입 올려놨고
[정종민] [오후 9:55] 사진자료는 카카오톡에서 받아서 사용하시면 될꺼같아요
[정종민] [오후 9:55] 더 좋은 아이디어 있으시면 수정해주세요
--------------- 2020년 5월 4일 월요일 ---------------
[김지훈] [오후 8:09] 깃헙에 .gitignore 랑 모델들어있던 폴더는 일부러 지운거인가요
[정종민] [오후 8:10] 카카오톡에 사진 올려둔거면 될꺼에요
[정종민] [오후 8:10] 나머지는 그 코드 구동시키느라 쓴거라서
[김지훈] [오후 8:11] 깃헙에는 그냥 코드하나만 올려놓을건가요?
[정종민] [오후 8:11] 네
[정종민] [오후 8:11] test.dlib
[김지훈] [오후 8:12] 바로 클론하면 실행도 되게 하는게 좋을거같은데
[정종민] [오후 8:13] 수정할까요?
[김지훈] [오후 8:14] 편한대로 하면되긴하는데
[김지훈] [오후 8:14] 다른분들은 어떠신지 모르겠네
[정종민] [오후 8:15] 저 코드
[정종민] [오후 8:15] 쓸데없는게 너무 많이 나오는데
[정종민] [오후 8:15] 일단 사진 크기부터 크게 만드는 방법 찾고
[정종민] [오후 8:15] 감정은 텐서플로우가 이미 지정해줬으니
[정종민] [오후 8:15] happy = background_happy 뭐 이런씩으로 지정해서 하는게 좋을꺼같네요
[김지훈] [오후 8:16] 쓸데없는게 어떤 거 말씀이세요?
[원종윤] [오후 8:16] 제가 보낸 코드 얘기하시는 건가요..?
[정종민] [오후 8:16] 아니요 ㅋㅋ;
[정종민] [오후 8:16] 그 감정퍼센트하고
[정종민] [오후 8:16] grabcut하기전에 뜨는 과정
[정종민] [오후 8:16] 생략처리하는게 더 깔끔하다 싶다 해서요
[정종민] [오후 8:17] 아니면 생기부에 보여주기 위함이니 차라리 과정을 다 넣을까요?
[김지훈] [오후 8:19] 음 배경합치는거 까진 완성된건가요
[정종민] [오후 8:19] 네
[김지훈] [오후 8:20] 그럼 그 퍼센트 비율에 맞춰서 배경을 바꿔보는건 어때요
[김지훈] [오후 8:20] 어느정도 행복한지 슬픈지
[김지훈] [오후 8:20] 배경으로 나타내는걸로
[정종민] [오후 8:20] 네 그게 첫번째 과정이고
[정종민] [오후 8:20] 그거 잘되면 인공지능 써서 웹서핑 시킨다음 바꿔보는걸로 하죠
[김지훈] [오후 8:21] 그럼 그렇게하고 아직 시간이 많이남았으니까
[김지훈] [오후 8:21] 감정인식 인공지능을
[김지훈] [오후 8:21] 직접 만들어보는건 어떨까요
[정종민] [오후 8:21] 네 이게 프로토타입이니까
[정종민] [오후 8:21] 보완할꺼 최대한 보완하면 될꺼같아요
[정종민] [오후 8:24] 저는 인공지능 조사해볼테니까 다른분들도 같이 인공지능 조사해주시던지 아니면 사진크기 키우는법이나 grabcut 좀 더 선명하게 뜨는 법 조사해주시면 될꺼같습니다
[김지훈] [오후 8:25] 저도 인공지능 조사하겠습니다
[최연석] [오후 8:32] cv2.resize(원본, dsize=(0, 0),가로배수,세로배수, interpolation=cv2.INTER_LINEAR)
[정종민] [오후 8:32] grabcut에 영향을 안주나요?
[최연석] [오후 8:33] grabcut하기전에 이미지 덮어쓰면 될거 같아요
[정종민] [오후 8:33] 글쎄요...
[정종민] [오후 8:34] 텐서플로우 신경망 인식 초과 범위 오류인데
[정종민] [오후 8:34] 이걸 키워야 해결될듯하네요
[원종윤] [오후 8:45] 사진크기 키우는 이유가 뭔가요..?
[정종민] [오후 8:45] 저희 목적은 셀카급 크기인데
[정종민] [오후 8:45] 사진
[정종민] [오후 8:46] 지금 크기가 이만해요
[원종윤] [오후 8:46] 얼굴이 너무 배경에 비해 작아서 그런건가요
[정종민] [오후 8:46] 네
[정종민] [오후 8:47] 얼굴 크기를 좀 크게 해야하지 않을까 싶네요
[원종윤] [오후 8:54] 크기를 무작정 크게 하는게 아니라 배경 비율에 맞춰서 키워야 할 것 같아요. 그리고 프로토타입 돌려봤는데, 저 얼굴 들어있는 이미지 말고 다른 이미지에서는 결과값이 안나오는데, 저만 그런지 알고싶습니다
[정종민] [오후 8:54] 말씀드렸다시피 지금 600X440인가 사이즈 이상 오바되면 인식을 못합니다
[정종민] [오후 8:55] 사진 작은거 가져오시면 인식이 가능하실껍니다
[원종윤] [오후 8:55] 아..알겠습니다.
[정종민] [오후 8:55] 그리고 크게 하려면 어떻게든 텐서플로우 신경망을 늘려야 할꺼 같아요
[정종민] [오후 8:56] 아니면 배경을 작게해서 얼굴에 맞추는수밖에 없는데
[정종민] [오후 8:56] 그건 화소나 화질 다깨지지 않을까요
[정종민] [오후 9:18] 사진
[정종민] [오후 9:18] 배경 잘 바뀝니다
[정종민] [오후 9:18] 이제 얼굴 흉상까지 따기/ 얼굴 크기 키우기/ 인공지능이 배경찾게 하기 
[정종민] [오후 9:18] 하고 코드 공부 좀 하면 될꺼같네요
[정종민] [오후 9:33] if문으로 배경 변경할 수 있는 코드 올려놨습니다
--------------- 2020년 5월 6일 수요일 ---------------
[정종민] [오후 8:27] 각자 뭐 조사하세요?
[원종윤] [오후 8:28] 얼굴 이미지 확대하는거 시도하고 있습니다.
[원종윤] [오후 8:39] 사진
[원종윤] [오후 8:40] 확대 성공했습니다
[정종민] [오후 8:40] 깃허브에
[정종민] [오후 8:40] 수정해주실수 있나요
[원종윤] [오후 8:40] 잠시만요
[원종윤] [오후 8:41] 사진
[원종윤] [오후 8:41] 에딧 여기서 하는거 맞나요
[정종민] [오후 8:42] 잠시만요
[정종민] [오후 8:42] 권한 풀게요
[정종민] [오후 8:47] 수정 안되나요?
[원종윤] [오후 8:48] 아직 안되요
[원종윤] [오후 9:40] 아직 권한 못푸신거지요..?
[정종민] [오후 9:40] 예
[정종민] [오후 9:41] push 설정 잘못해서
[정종민] [오후 9:41] 꼬인듯요
[정종민] [오후 9:41] 텐서플로우 오류 해결하고 할게요
[원종윤] [오후 9:41] 넵
--------------- 2020년 5월 11일 월요일 ---------------
[정종민] [오후 8:20] 깃허브 옮겨야하는데
[정종민] [오후 8:20] 메일 한번씩 알려주세요
[김지훈] [오후 8:21] rlawlgns51796@gmail.com
[원종윤] [오후 8:21] noel8044@naver.com
[찬울] [오후 8:23] occ1433@gmail.com
[최연석] [오후 9:05] cys08190819@gmail.com
[정종민] [오후 9:35] https://baram4815.tistory.com/entry/OpenCV-4%EA%B4%80%EC%8B%AC-%EC%98%81%EC%97%AD
[정종민] [오후 9:35] C언어인데 이것처럼 관심영역 지정하는법은 없을까요
[원종윤] [오후 9:49] 이거 성공했습니다
[정종민] [오후 9:50] 저거 제가 전에 했을텐데
[원종윤] [오후 9:50] 아 그런가요
[김지훈] [오후 9:50] 결과화면좀 보여주세요
[정종민] [오후 9:51] import cv2
import numpy as np
from keras.preprocessing.image import img_to_array
from keras.models import load_model
import dlib
#dlib를 상용 하는 이유는 XML파일을 이용하는 것 보다 dlib를 사용하여 인식 하는 것이 더 인식이 잘되는 듯 함
#dlib를 설치하기 위해서는 Anaconda에서 pip install cmake -> pip install dlib 설치


# dlib얼굴 인식 모델, 감정인식 모델 불러오기
#face_detection = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml') #사용 안함
detector = dlib.get_frontal_face_detector() #얼굴 인식을 위한 dlib 기본 안면인식 모델
emotion_classifier = load_model('./emotion/emotion_model.hdf5', compile=False)
EMOTIONS = ["Angry", "Disgusting", "Fearful", "Happy", "Sad", "Surpring", "Neutral"]


# 이미지 불러오기
frame = cv2.imread('./SampleImage/peoples.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((1200, 720, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

background_sad = cv2.imread('./SampleImage/background_sad.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((600, 500, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

background_happy = cv2.imread('./SampleImage/background_happy.jpg')
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
canvas = np.zeros((600, 500, 3), dtype="uint8")
dets = detector(frame, 1) #얼굴인식 완료

# Perform emotion recognition only when face is detected
if len(dets) > 0:
    # Resize the image to 48x48 for neural network
    for k, d in enumerate(dets): # k = 얼굴 인식 갯수, d = 좌표(?)
        fX = d.left()
        fY = d.top()
        fW = d.right()
        fH = d.bottom()
        print(fX, fY, fW, fH)

        # grabcut
        rect = (fX, int((fY - fY / 2)), fW, fH)
        img = frame.copy()
        img2 = img.copy()
        mask = np.zeros(img.shape[:2], dtype=np.uint8)
        output = np.zeros(img.shape, np.uint8)

        bgdModel = np.zeros((1, 65), np.float64)
        fgdModel = np.zeros((1, 65), np.float64)

        cv2.grabCut(img2, mask, rect, bgdModel, fgdModel, 1, cv2.GC_INIT_WITH_RECT)
        mask2 = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')
        output = cv2.bitwise_and(img2, img2, mask=mask2)


        # --grabcut

        roi = gray[fY:fH, fX:fW] #얼굴 인식된 좌표
        roi = cv2.resize(roi, (64, 64))
        roi = roi.astype("float") / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)

        # Emotion predict
        #감정 인식하기
        preds = emotion_classifier.predict(roi)[0]
        emotion_probability = np.max(preds)
        label = EMOTIONS[preds.argmax()]

        # Assign labeling
        cv2.putText(frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
        cv2.rectangle(frame, (fX, fY), (fW, fH), (0, 0, 255), 2)

    # Label printing
    #사람이 여려명일 때는 마지막으로 감정을 인식한 사람의 감정 상태만 프린트함
    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):
        text = "{}: {:.2f}%".format(emotion, prob * 100)
        w = int(prob * 300)
        cv2.rectangle(canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 0, 255), -1)
        cv2.putText(canvas, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)


# 이미지 지정
# EMOTIONS = ["Angry", "Disgusting", "Fearful", "Happy", "Sad", "Surpring", "Neutral"]

if EMOTIONS[3]:
    background = cv2.imread("./SampleImage/background_happy.jpg")
elif EMOTIONS[4]:
    background = cv2.imread("./SampleImage/background_sad.jpg")
else:
    background = cv2.imread("./SampleImage/background_surprising.jpg")
logo = output

gray_logo = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)
_, mask_inv = cv2.threshold(gray_logo, 10, 255, cv2.THRESH_BINARY_INV)

cv2.imshow("mask_inv", mask_inv)
cv2.waitKey()

background_height, background_width, _ = background.shape # 900, 600, 3
logo_height, logo_width, _ = logo.shape # 360, 313, 3

x = (background_height - logo_height) //2
y = (background_width - logo_width) // 2


roi = background[x: x+logo_height, y: y+logo_width]

cv2.imshow("ROI", roi)
cv2.waitKey()

roi_logo = cv2.add(logo, roi, mask=mask_inv)
cv2.imshow("roi_logo", roi_logo)
cv2.waitKey()

result = cv2.add(roi_logo, logo)
cv2.imshow("result", result)
cv2.waitKey()

np.copyto(roi, result)
cv2.imshow("result_background", background)
cv2.waitKey()


# Open two windows
## Display image ("Emotion Recognition")
## Display probabilities of emotion
cv2.imshow('output', output)
cv2.imshow('Emotion Recognition', frame)
cv2.imshow("Probabilities", canvas)
cv2.waitKey(0)



# Clear program and close windows
cv2.destroyAllWindows()
[정종민] [오후 9:51] 같은가 한번 확인해보세요
[원종윤] [오후 9:54] 그때 이부분 수정하신거 아닌가요..?
[원종윤] [오후 9:54] 사진
[정종민] [오후 9:54] 예 그럴껄요?
[김지훈] [오후 9:55] 깃헙에 지금 코드 업로드 돼있어요?
[정종민] [오후 9:55] 해놨었는데
[정종민] [오후 9:55] team을 제 개인 repository로 잘못 만들어서
[정종민] [오후 9:55] 다시 만들었었어요 지난번에
[원종윤] [오후 9:55] 이거 퍼센티지에 따라서 바뀌는거 아니지 않나요..? 저는 text에서 제일 높은 퍼센티지만 추출에서 범위 분류 했어요
[정종민] [오후 9:56] 다음에 확인해볼게요
[원종윤] [오후 9:56] 넵
--------------- 2020년 5월 13일 수요일 ---------------
[정종민] [오후 8:27] 지난번에
[정종민] [오후 8:27] 퍼센트 따라서 바뀐다는거
[원종윤] [오후 8:27] 네
[정종민] [오후 8:28] 그거 테스트 해볼게요
[정종민] [오후 8:34] 혹시 저희
[정종민] [오후 8:34] 다른 사람얼굴 샘플있나요
[정종민] [오후 8:34] 사진
[정종민] [오후 8:34] 이사진말고
[원종윤] [오후 8:34] 사진
[원종윤] [오후 8:34] 이거 되요
[정종민] [오후 8:35] 웃는거말고
[정종민] [오후 8:35] 없나요
[정종민] [오후 8:35] ?
[정종민] [오후 8:35] 감정 테스트해볼꺼라
[원종윤] [오후 8:35] 사진
[원종윤] [오후 8:35] 이건 부분적으로 되는것 같아요
[정종민] [오후 8:38] 정말 안되네
[정종민] [오후 8:38] 만드셨다던 코드좀 올려주세요
[원종윤] [오후 8:38] 퍼센트 추출하는거요 아니면 확대한는거요..?
[정종민] [오후 8:38] 퍼센트부터 보완하죠
[원종윤] [오후 8:39]  # Label printing
    #사람이 여려명일 때는 마지막으로 감정을 인식한 사람의 감정 상태만 프린트함
    maxnum = 1
    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):

        text = "{}: {:.2f}%".format(emotion, prob * 100)
        if maxnum < prob * 100:
            maxnum = int(prob * 100)
        w = int(prob * 300)
        cv2.rectangle(canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 0, 255), -1)
        cv2.putText(canvas, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)

print(maxnum)
[원종윤] [오후 8:39] 이 부분만 바꾸시면 되요
[원종윤] [오후 8:40] 그냥 간단하게 최대값 빼내게 만들엇어요
[정종민] [오후 8:41] 저 코드 바꾸면
[정종민] [오후 8:42] 감정list 해당되는게 프린트되야하나요>
[정종민] [오후 8:42] 별 변화가 없는데
[원종윤] [오후 8:43] maxnum이 그 사진 넣으면 happy의 퍼센티지 값이고, maxnum에 따라서 범위를 설정해야 사진이 바뀌어요
[정종민] [오후 8:45] maxmum 이란 변수가
[정종민] [오후 8:46] 그 얼굴에 나타난 감정이라고요?
[원종윤] [오후 8:48] 사진
[원종윤] [오후 8:48] 정수값으로 나오게 해놨어요
[정종민] [오후 8:50] 퍼센티지 나오게 해서
[정종민] [오후 8:51] 좋은점이..?
[원종윤] [오후 8:51] 제안하셔서 해본거죠
[정종민] [오후 8:52] 바꿀수 있으면 좋긴 한데
[정종민] [오후 8:52] 퍼센트 비율로 바꾸기보다는 list에 있는 값 갖고와서 하는게 빠르지않나요>
[정종민] [오후 8:52] 저도 배경 바꾸는건 구현을 못하겠음
[원종윤] [오후 8:54] list 가 emotion recognition 창에 들어가는 텍스트 정보 들어있는거 말씀하시는 거지요..?
[정종민] [오후 8:54] # EMOTIONS = ["Angry", "Disgusting", "Fearful", "Happy", "Sad", "Surpring", "Neutral"]
[정종민] [오후 8:54] 이거요
[원종윤] [오후 8:58] emotion 값에 따라서 사진이 바뀌는 것은 당연한거고, 제가 쓴 코드는 emotion 배열에 있는 감정에 따르고 그 감정에서 퍼센트 범위를 나눠서 그 범위에 따라 사진을 다르게 할 수 있도록 하기 위해서 퍼센트 값을 추출한거에요
[정종민] [오후 9:08] 퍼센트값 = 사진 으로 설정시켜서 사진을 바꿀 수 있을까요?
[정종민] [오후 9:09] 퍼센트쪽은 잘 모르겠네요
[원종윤] [오후 9:09] 일단 알겠습니다 다른거부터 하시죠
[정종민] [오후 9:10] 사진 확대는
[정종민] [오후 9:10] 사진 용량을 늘려서 집어넣을수 있게 만드신건가요?
[원종윤] [오후 9:12] 아니요 감정인식하는 그 파일이 적은 용량밖에 인식 못한다고 하셔서 확대하는걸로 만들었습니다
--------------- 2020년 5월 18일 월요일 ---------------
[정종민] [오후 8:51] 다들 진전들 있으신가요?
[김지훈] [오후 8:51] 전 아직 이제 막 학습 해보는중이에요
[정종민] [오후 8:53] 저희
[정종민] [오후 8:53] 해야할꺼 정리해보는게 어떨까요?
[정종민] [오후 8:53] 지금 하고있는게 조금 난잡해서 햇깔리는데 저만 그런가요
[김지훈] [오후 8:54] 지금은 각자 뭘하고 계신가요
[원종윤] [오후 8:55] 몸 인식 알고리즘을 아직 못찾아서 일단 조금이라도 인식 되는 haarcascades로 인식한 좌표 grabcut 해서 출력할 수 있도록 만드는 중이에요, 나중에 알고리즘 찾으면 적용할려고요
[정종민] [오후 8:55] 저도 흉곽쪽까지 구현하는거 찾아보고 있는데 여간 쉽지 않네요
[정종민] [오후 8:55] 알고리즘을 수정하던가 해야하는데 별 진전이 없습니다...
[김지훈] [오후 8:57] 흠 흉상을 인식하는 학습데이터가 있나요?
[원종윤] [오후 9:48] grabcut이 얼굴을 타겟으로 배경과 얼굴을 구분하는 거였나요..?
[정종민] [오후 9:49] 얼굴이라기 딱 정하기는 애매합니다
[정종민] [오후 9:49] 지정한 부분에서 명암차이로 뽑아내는 기법일꺼에요
[원종윤] [오후 9:50] 제가 몸 상단 부분까지 있는 사진에서 grabcut 했더니 이렇게 나와서요..
[원종윤] [오후 9:50] 사진
[정종민] [오후 9:51] 부분지정이 자동인가요 수동인가요?
[정종민] [오후 9:51] 자동이면 아마 얼굴쪽밖에 지정을 안해줘서 그럴껍니다
[원종윤] [오후 9:52] 저 그림이 haarcascade로 몸 상단부가 인식이 되서 그 좌표를 추출해서 grabcut에 넣었어요
[정종민] [오후 9:54] 원본을
[정종민] [오후 9:54] 볼수 있나요
[원종윤] [오후 9:54] 사진
[정종민] [오후 9:54] 범위지정이
[정종민] [오후 9:54] 맞는거같아요
[정종민] [오후 9:54] 아니면 명암구별이 옷이랑 잘 안된다거나?
--------------- 2020년 5월 20일 수요일 ---------------
[원종윤] [오후 8:34] 혹시 아시는 몸 검출하는 알고리즘 있으신가요..?
[정종민] [오후 8:35] 보행자 검출을 opencv로 구현한 샘플을 찾았는데
[정종민] [오후 8:35] 이게 도움될꺼 같으시면 올려드릴게요
[원종윤] [오후 8:35] 네 올려주세요
[정종민] [오후 8:36] 파일: PedestrianOpenCV.zip
[정종민] [오후 8:36] dll은 시스템 파일이라
[정종민] [오후 8:36] 링크 올려드릴게요ㅛ
[정종민] [오후 8:37] https://drive.google.com/file/d/1LeXIhJeQFXOPonLq8AIF2eEg1ZKU9xF4/view?usp=sharing
[정종민] [오후 8:54] https://www.morethantechnical.com/2010/05/05/bust-out-your-own-graphcut-based-image-segmentation-with-opencv-w-code/
[정종민] [오후 8:54] 위 사이트 보시면
[정종민] [오후 8:54] 흉각이 안나오는 힌트를 얻을수 있을꺼 같네요
[정종민] [오후 9:20] 저희 처음에 코드 샘플에
[정종민] [오후 9:20] 동영상까지 포함되있지 않았었나요
[정종민] [오후 9:20] ?
[김지훈] [오후 9:20] 동영상이 있었는데 
[김지훈] [오후 9:20] 동영상으로 dlib 인식이랑 그랩컷 쓰니까
[김지훈] [오후 9:20] 렉이 너무 심하더라구요
[정종민] [오후 9:20] 그럴싸하게
[정종민] [오후 9:20] 되긴 하나요?
[정종민] [오후 9:21] 얼굴이 짤린다던지 그런건 없이
[김지훈] [오후 9:21] 그랩컷이 잘 안돼요
[정종민] [오후 9:21] 제가 테스트 한번 해보겠습니다
[정종민] [오후 9:21] 감정인식이나 그랩컷을 연속적으로 하면 랙이 걸릴꺼같긴 하네요
[정종민] [오후 9:34]         roi = gray[fY:fH, fX:fW] #얼굴 인식된 좌표
        roi = cv2.resize(roi, (64, 64))
        roi = roi.astype("float") / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)
[정종민] [오후 9:34] Roi가 뜻하는게 뭔가요?
[김지훈] [오후 9:34] 이미지를 좌표크기만큼 자르고 감정인식을 할수 있게끔 데이터를
[김지훈] [오후 9:34] 바꾼거같네요
[김지훈] [오후 9:35] 감정인식 모델을 사용하려면 이미지파일을 그 모델에 맞게 해줘야 되더라구요
--------------- 2020년 5월 25일 월요일 ---------------
[정종민] [오후 8:41] hdf5(Hierarchical Data Format version 5)
[정종민] [오후 8:41] 이거 알고계신가요?
[김지훈] [오후 8:41] hdf5 파일은 모델파일 아닌가요
[김지훈] [오후 8:42] 저희 감정인식한느모델파일도
[김지훈] [오후 8:42] hdf5
[김지훈] [오후 8:42] 파일이에요
[정종민] [오후 8:42] 근데
[정종민] [오후 8:42] 이거 설명보면
[정종민] [오후 8:42] 대용량 복합 데이터 저장에 쓰면서 대용량의 이미지 데이터나 항공기 선박등에 쓰인다는데
[정종민] [오후 8:42] haarcascade처럼 
[정종민] [오후 8:43] 다른 파일명도 나올수 있는거 아닌가 싶어서요
[김지훈] [오후 8:43] 다른파일명 많을거에요
[김지훈] [오후 8:43] 지금 다른모델들 사용하고이쓴데
[정종민] [오후 8:43] 보통 알고리즘 파일명 하면
[김지훈] [오후 8:43] 이름이 다르더라구요
[정종민] [오후 8:43] xml이 많지 않나요?
[정종민] [오후 8:43] 저런 파일형식은 처음봐서요
--------------- 2020년 5월 27일 수요일 ---------------
[정종민] [오후 9:01] 사진 크기가 커지면
[정종민] [오후 9:02] 오류뜨는 부분이 grabcut의 output is not defined 맞는지 확인해주세요
[김지훈] [오후 9:03] 사진
[김지훈] [오후 9:03] 저는 일단 결과 대충
[김지훈] [오후 9:04] 나오는거같앙
[김지훈] [오후 9:04] 요
[김지훈] [오후 9:04] https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/
[김지훈] [오후 9:04] 여기 코드 이용해서
[김지훈] [오후 9:05] 그랩컷 해본거에요 한번씩 봐보세요
[정종민] [오후 9:05] 신경망이 작아서 큰 사진이 안된다고 생각했는데
[정종민] [오후 9:05] 그게 아닌가요?
[김지훈] [오후 9:05] 신경망이 작은거는
[김지훈] [오후 9:05] 신경망을 이용할떄만 사진크기를
[김지훈] [오후 9:05] resize하면
[김지훈] [오후 9:05] 되지않나요?
[김지훈] [오후 9:06] 신경망을 이용하는건 얼굴인식이나 감정인식할때 밖에 안쓸텐데
[김지훈] [오후 9:06] 제가 감정인식 모델 만들었을때 데이터 가공할때 사진 크기 64,64로 resize하고 했었어요
[원종윤] [오후 9:07] 그랩컷 좌표는 뭘로 잡으신거에요..?얼굴좌표는 아닌것 같은데..
[정종민] [오후 9:07] 좌표는
[정종민] [오후 9:07] 알아서 잡아주지 않나요
[김지훈] [오후 9:07] 그랩컷좌표는 얼굴로 안하고 사진크기보다 좀더 작게
[김지훈] [오후 9:07] 해서 했어요
[정종민] [오후 9:07] 일단 위 올려주신 코드좀 봐야
[정종민] [오후 9:07] 뭔지 알겠네요
[김지훈] [오후 9:08] 지금 코드정리중이에요
[김지훈] [오후 9:09] 일단 제가 해본건 그랩컷을 처음에 먼저한다음에 몸에 랜드마크점찍고 선연결하고 그 선을 주변을cv2.GC_INIT_WITH_MASK로 그랩컷을 한번 더해준거에요
[김지훈] [오후 9:10] 아 이거 코드 사용할려면
[김지훈] [오후 9:10] https://m.blog.naver.com/PostView.nhn?blogId=rhrkdfus&logNo=221531159811&proxyReferer=https:%2F%2Fwww.google.com%2F
[김지훈] [오후 9:10] 여기서 설치하라는거 해야되는데
[김지훈] [오후 9:10] 좀 오래걸리거든요
[김지훈] [오후 9:10] 그래도 해보실래요?
[정종민] [오후 9:11] 수동지정인가요
[정종민] [오후 9:11] ?
[김지훈] [오후 9:11] 그게 무슨말이죠/
[정종민] [오후 9:11] 아닙니다
[정종민] [오후 9:11] 일단 설치해볼게요
[김지훈] [오후 9:11] 코드 정리해서 올려드릴게요
[김지훈] [오후 9:19] 파일: grabcut_Test.txt
[김지훈] [오후 9:19] 설치다되면 경로바꿔서 해보세요
[김지훈] [오후 9:35] 아그리고 이코드 배경넣는거는 임의로 아무거나 넣어놓은거에요
[김지훈] [오후 9:35] 혹시 감정에맞는 배경 사진들 갖고계신가요
[원종윤] [오후 9:36] 사진
[원종윤] [오후 9:36] 사진
[원종윤] [오후 9:56] 혹시 svm 학습시키는 법 자료있으신 분 있나요..?있으시면 다음시간에 보내주세요..?
--------------- 2020년 6월 1일 월요일 ---------------
[정종민] [오후 8:23] 오픈포즈가
[정종민] [오후 8:23] 정확히 뭔지 알려주실수 있나요?
[정종민] [오후 8:24] 얼핏 보기에는 대용량 파이썬 확장 라이브러리 같은데
[정종민] [오후 8:24] 맞나요
[김지훈] [오후 8:24] OpenPose : Caffe와 OpenCV를 기반으로 구성된 손, 얼굴 포함 몸의 움직임을 추적해주는 API
[김지훈] [오후 8:24] 라고 돼있네요
[정종민] [오후 8:29] 나중에 보고서 작성할때 
[정종민] [오후 8:30] cascade hd5 api 잘 구분해서 적어야 할꺼 같네요
[정종민] [오후 8:30] 위에 테스트해보신 얼굴 사진좀 알려주시겠나요
[김지훈] [오후 8:31] 사람얼굴 원본사진이요?
[정종민] [오후 8:31] 네네
[김지훈] [오후 8:31] 사진
[정종민] [오후 8:33] protoFile = "C:/Users/EUNMI/Desktop/openpose-master/openpose-master/models/pose/coco/pose_deploy_linevec.prototxt"
weightsFile = "C:/Users/EUNMI/Desktop/openpose-master/openpose-master/models/pose/coco/pose_iter_440000.caffemodel"
[정종민] [오후 8:34] 자기 컴퓨터에 맞게 수정해야 하는 부분
[정종민] [오후 8:34] 맞죠?
[김지훈] [오후 8:34] 네 맞아요
--------------- 2020년 6월 3일 수요일 ---------------
[정종민] [오후 8:41] 사진
[정종민] [오후 8:41] 이리뜨는거 맞나요
[김지훈] [오후 8:49] 맨처음 그랩컷 하는법을 바꿔야 할거같아요
[김지훈] [오후 8:50] 사진마다 되는것도있고 안되는것도 있어서
[원종윤] [오후 8:50] 그랩컷 정확도를 올리기 위해서 방법을 바꿔야한다는 말씀이시죠..?
[김지훈] [오후 8:51] 네
[정종민] [오후 8:51] 아프시다던데 건강 조심하세요
[김지훈] [오후 8:51] 지금은 좀 괜찮아요ㅎ
[정종민] [오후 8:52] 이제 배경 바꾸는거하고 동영상만 하면 대강 마무리 되는거 같네요
[원종윤] [오후 8:52] 선생님께서 조금씩 보완하고, 동영상으로 구현하는 쪽으로 가자고 하셨어요
--------------- 2020년 6월 8일 월요일 ---------------
[정종민] [오후 9:00] openpose 가 원래 키넥트센서라는 것으로 구동해야 하는거 같은데 그걸 일반캠으로 하게 만들어준 라이브러리 같아요
[정종민] [오후 9:00] https://eehoeskrap.tistory.com/236
[김지훈] [오후 9:35] 지금 영상으로 대충 해봤는데
[김지훈] [오후 9:35] 동영상
[김지훈] [오후 9:35] 결과는 대충 나오거든요
[김지훈] [오후 9:35] 근데 이 4초짜리 영상만드는데 5분정도 걸려요
[김지훈] [오후 9:36] 이게 프레임하나마다 일일이 배경자르고 하는거라
[김지훈] [오후 9:36] 영상으로 할려면 방법좀 많이 바꿔야할거같아요
[원종윤] [오후 9:44] 프레임 수 줄이는건 별로일까요..?
[김지훈] [오후 9:46] 프레임수를 줄이면 영상품질자체가 안좋아지고 애초에 1프레임당 처리하는시간이 3초정도 걸려서 별로일거같아요
[김지훈] [오후 9:46] 저영상만 초당프레임이 30fps 에요
[김지훈] [오후 9:47] 1초 처리하는데 90초 걸리는거죠
[정종민] [오후 9:47] 저 영상도 openpose 적용 시킨건가요?
[김지훈] [오후 9:47] 네 다 적용시켜서 배경합성한거만 저장한거에요
[정종민] [오후 9:48] openpose 관련해서 좀 찾아보고 있는데 원래 키네틱 센서로 하는걸 그냥 이미지로 처리하는거니까 성능은 좀 딸릴수 밖에 없는거 같아요
[정종민] [오후 9:48] densepose라는것도 있는데 해결방법 찾아볼게요
[김지훈] [오후 9:48] 그 오픈포즈 쓰기전에도 영상으로 해봤었는데
[김지훈] [오후 9:48] 그랩컷이 성능을
[김지훈] [오후 9:49] 쭉 떨어트리더라구요
[김지훈] [오후 9:49] 근데 dlib 도 그렇고 여러가지 라이브러리랑 api 합쳐서쓸려니까
[김지훈] [오후 9:49] 더 떨어트리는거같아요
[최연석] [오후 9:50] 그래서 연산량 줄이기 위해 추적 알고리즘들 쓰더라구요 
[김지훈] [오후 9:51] 관련 자료 가 있나요
[최연석] [오후 9:53] MRF modeling? 자세히 모르겠어요
[김지훈] [오후 9:55] 보셨던 자료 링크좀 보내주세요
[최연석] [오후 9:56] https://blog.naver.com/opendori/80060282847
[최연석] [오후 9:56] http://koreascience.or.kr/article/JAKO201436351075109.pdf
--------------- 2020년 6월 10일 수요일 ---------------
[찬울] [오후 2:58] 내일이 시험이라 오늘만 8시에 참여 못 할거같습니다. 죄송합니다.
--------------- 2020년 6월 15일 월요일 ---------------
[정종민] [오후 2:16] 고3 시험기간이라 이번주 다음주 출석 힘들꺼 같습니다...죄송합니다
--------------- 2020년 6월 17일 수요일 ---------------
[김지훈] [오후 8:44] 저희 일단 배경을 먼저 제거를 잘 할려면 원본 배경이 크로마키처럼 되있어야 할거같아요
[김지훈] [오후 8:44] 동영상
[김지훈] [오후 8:44] 이런식으로
[김지훈] [오후 8:44] 그리고 그랩컷 대체할걸로
[김지훈] [오후 8:45] watershed알고리즘
[김지훈] [오후 8:45] 사용하고있는데
[김지훈] [오후 8:45] 성능은 그랩컷보다는 떨어지는데 시간면에서는 더 좋아요
[김지훈] [오후 8:45] 혹시 다른 알고리즘 찾아보신분
[김지훈] [오후 8:45] 계신가요
[김지훈] [오후 8:49] 파일: watershed_test.txt
[김지훈] [오후 8:50] 저기서 써먹을거 써먹으면 될거같아요
[김지훈] [오후 8:50] 아마 10번째 줄에 thr이나 39번째 줄에 h 둘중에 써먹으면 될거같아요
[원종윤] [오후 9:05] https://webnautes.tistory.com/1248
[원종윤] [오후 9:06] 이런것도 있는데, 조금 느린것 같아요
[김지훈] [오후 9:47] 저런것들 응용해서 해보시고 결과나오면 결과 더 잘나오는걸로 하는걸로해요
[김지훈] [오후 9:47] 사진
[김지훈] [오후 9:48] 파일: watershed_test2.txt
[김지훈] [오후 9:48] 일단 저는 저 result에 잔부분만 보완해주면 될거같아요
--------------- 2020년 6월 22일 월요일 ---------------
[김지훈] [오후 8:09] 파일: watershed_test2.txt
[원종윤] [오후 8:15] 시간 많이 걸려서 haarcascade로 바꾸신거에요..?
[김지훈] [오후 8:16] 네 dlib는 렉이 많이 걸려서 바꿨어요
[원종윤] [오후 9:21] haarcascades - 적분
openpose
watershed or Background Subtraction 알고리즘
opencv 이미지 형식 변화
감정인식
[원종윤] [오후 9:22] 사용 기술 이정도 맞나요
[김지훈] [오후 9:22] openpose 는 지금 없애긴 했어요
[김지훈] [오후 9:27] 여태까지 사용했다가 성능이랑 시간면에서 없앤거는 dlib 얼굴인식, grabcut, openpose 가 있네요
[김지훈] [오후 9:30] 파일: watershed_TestVideo.zip
[김지훈] [오후 9:30] 영상 여러개로 테스트좀
[김지훈] [오후 9:30] 해봤는데
[김지훈] [오후 9:30] 되는게 있고 안되는게있어서 
[김지훈] [오후 9:31] 원인도 찾아보고 개선해보는게 좋을거같아요
--------------- 2020년 6월 24일 수요일 ---------------
[원종윤] [오후 8:47] 저희 프로젝트랑 교과 연계되는 부분 구글문서에 적어주실수 있으신가요..? 제가 2학년이여서 교과쪽을 잘 모르겠네요..
--------------- 2020년 6월 29일 월요일 ---------------
[정종민] [오후 8:13] 코드 마지막으로 수정한게
[정종민] [오후 8:14] 전에 올려주셨던 건가요
[김지훈] [오후 8:14] 네
[정종민] [오후 8:15] 제가
[정종민] [오후 8:15] 잘 모르는 걸수도 있는데
[정종민] [오후 8:15] haarcascade는 미분이랑 크게 관련이 없는걸로 알아요
[정종민] [오후 8:16] 전에 학교 보고서 작성해서 제출할때가 있었는데 라플라시안이나 케냐 기법 이런 테두리 틀따기는 관련이 있는데
[정종민] [오후 8:16] haarcascade는 명암구분기법 아닌가요?
[원종윤] [오후 8:17] 그런가요..?아직 미분을 안배워서 잘 모르겠습니다
[정종민] [오후 8:17] ㅇㅎ...
[정종민] [오후 8:19] 저희 subtraction 알고리즘 쓰인 코드부분 알려주실수 있나요
[김지훈] [오후 8:24] subtraction을 쓴적이 있나요
[정종민] [오후 8:24] haarcascades - 적분
openpose
watershed or Background Subtraction 알고리즘
opencv 이미지 형식 변화
감정인식
[정종민] [오후 8:24] 위에 이렇게 적으셨던데
[정종민] [오후 8:24] 세번째꺼는
[원종윤] [오후 8:24] watershed 썻어요
[정종민] [오후 8:24] 잘 모르겠어서요
[원종윤] [오후 8:27] import cv2
import numpy as np



cap = cv2.VideoCapture("C:/Users/user/PycharmProjects/OpenCV/doc/chroma key.mp4")

# 옵션 설명 http://layer0.authentise.com/segment-background-using-computer-vision.html
fgbg = cv2.createBackgroundSubtractorMOG2(varThreshold=100)


while(1):
    ret, frame = cap.read()

    fgmask = fgbg.apply(frame)



    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(fgmask)


    for index, centroid in enumerate(centroids):
        if stats[index][0] == 0 and stats[index][1] == 0:
            continue
        if np.any(np.isnan(centroid)):
            continue


        x, y, width, height, area = stats[index]
        centerX, centerY = int(centroid[0]), int(centroid[1])

        if area > 100:
            cv2.circle(frame, (centerX, centerY), 1, (0, 255, 0), 2)
            cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 0, 255))

    kernel = np.ones((3, 3), np.uint8)
    opening = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel, iterations=2)
    cv2.imshow('mask',fgmask)
    cv2.imshow('frame',frame)


    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()
[원종윤] [오후 8:28] 이건데, watershed 보다 잘 안따져서 안쓰고 있어요
[정종민] [오후 8:28] 아 알겠습니다